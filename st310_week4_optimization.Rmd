---
title: "Optimization for machine learning"
author: "Anastasia Chanbour"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # required
library(broom)     # required
library(modelr)    # for data_grid
library(GGally)
theme_set(theme_minimal(base_size = 22))
```


# 1-d smooth regression example

## Example data

Generate a one-dimensional example with a non-linear relationship

```{r}
# change this function
f <- function(x) sin(4*pi*x)
n <- 200
train1d <- 
  data.frame(
    x = rbeta(n, 1, 3)
    ) %>%
  # change the noise level sd
  mutate(y = f(x) + rnorm(n, sd = .1))

# plot data and loess curve
ggplot(train1d, aes(x, y)) + 
  geom_point() +
  geom_smooth()
```

Linear regression with a polynomial transform of x

```{r}
model_lm <- lm(y ~ poly(x, 5), data = train1d)

train1d_grid <- 
  data_grid(train1d,
          x = seq_range(c(x, 1), 500, expand = .05))

augment(model_lm,
        newdata = train1d_grid) %>%
  ggplot(aes(x, y)) +
  geom_point(data = train1d) +
  geom_line(aes(y = .fitted))
```

## Gradient descent

Goal: implement gradient descent and use it to solve for the coefficients of the above linear model

Hint: `model.matrix(model_lm)` can give you the predictor matrix.

Update is

$$
\beta_{k+1}=\beta_{k}-\gamma\nabla L(\beta_k)
$$

where $\gamma>0$ is the step size.

### Step 1: write functions to output the least squares loss and its gradient

```{r}
# Loss function in linear regression RSS:
least_squares_loss <- function(x,y,beta) {
  sum((y-x%*%beta)^2)
}

# Loss function gradient:
least_squares_gradient <- function(x,y,beta) {
  -2*t(x)%*%(y-x%*%beta) #in matrix notation
}
```

### Step 2: write a loop to take multiple steps in the direction of the negative gradient, keeping step size fixed

```{r}
# Model data and initialise coefficient vector
y <- train1d$y
x <- model.matrix(model_lm)
p <- ncol(x)

gamma <- 1 #step size within permitted values
beta0 <-rep(0,p) #initialise vector of parameters as 1xp vector of zeros 

previous_loss <- least_squares_loss(x,y,beta0)
grad0 <- least_squares_gradient(x,y, beta0)
beta1 <- beta0 - gamma*grad0 #first update of beta
next_loss <- least_squares_loss(x,y,beta1)
previous_beta <- beta1

for(i in 1:5){
  gradn <- least_squares_gradient(x,y, previous_beta)
  next_beta <- previous_beta - gamma*gradn
  previous_beta <- next_beta
  print(previous_beta)
}



```

### Step 3: write a function to step in the direction of the negative gradient until the loss function no longer decreases by a certain amount, keeping step size fixed

```{r}
gamma <- 0.001
beta0 <- beta0 <-rep(0,p)
previous_loss<- previous_loss <- least_squares_loss(x,y,beta0)
grad0 <- least_squares_gradient(x,y, beta0)
beta1 <- beta0 - gamma*grad0
next_loss <- least_squares_loss(x,y,beta1)
steps <- 1 #for printing updates


#we iterate until the change in values of the loss function is small enough
while ((next_loss - previous_loss > 0.01)) {
   gradn <- least_squares_gradient(x,y, previous_beta)
  next_beta <- previous_beta - gamma*gradn
  grad_current <- gradn
  if (steps %% 50 == 0) print(previous_loss)
  steps <- steps + 1
  previous_beta <- next_beta
  previous_loss <- next_loss
  next_loss <- least_squares_loss(x,y,next_beta)
  
}
  

```

### Step 4: experiment with manually decreasing the stepsize and convergence threshold

```{r}
c(steps, previous_loss, next_loss)
next_beta
```

### Step 5: use the Barzilai-Borwein method to choose step size

See https://en.wikipedia.org/wiki/Gradient_descent

```{r}

```

# Classification example

## Generate data

Which predictors have nonzero coefficients?

```{r}
n <- 100; p <- 4; sparsity <- 2
x <- matrix(rnorm(n*p), nrow = n)
true_beta <- c(rep(1, sparsity), rep(0, p - sparsity))
mu <- x %*% true_beta
px <- exp(mu)/(1+exp(mu))
y <- rbinom(n, 1, px)
train_hd <- data.frame(y = as.factor(y), x = x)
```

```{r}
train_hd %>% 
  select(y, num_range("x.", 1:4)) %>%
  ggpairs(progress = F)
```
## Implement gradient descent

First try this with logistic loss function

### Step 1: use numerical differential to approximate gradient

(see references below)

```{r}

```

### Step 5: full implementation

```{r}

```

### Check answer with results from `glm` (if possible, when p < n)

```{r}
glm(y ~ x, family = "binomial") %>%
  ggcoef()
```
# High dimensional regression and SGD

## Generate data

```{r}
n <- 100; p <- 200; sparsity <- 3
x <- matrix(rnorm(n*p), nrow = n)
true_beta <- c(rep(1, sparsity), rep(0, p - sparsity))
y <- x %*% true_beta
train_hd <- data.frame(y = y, x = x)
```

```{r}
train_hd %>% 
  select(y, num_range("x.", 1:6)) %>%
  ggpairs(progress = F)
```

## Implement SGD for least squares

See the references for hints

```{r}

```

Question: does it converge? There is no unique solution for least squares in high dimensional regression.

# Extra practice

- Modify the least squares loss function by adding a penalty on the coefficients, e.g. loss = MSE + L*beta^2, where L is a non-negative constant. Compute the new gradient and implement SGD with this loss function. Try various values of L on the same dataset, and implement "tuning" to find the best value of L 

- Implement SGD for logistic and hinge loss functions, and try it out on some examples of data that you generate where you know the true answer

# References

- https://mlstory.org/optimization.html for gradient descent, stochastic gradient descent, SGD quick start guide
- https://en.wikipedia.org/wiki/Numerical_differentiation for symmetric difference quotient, step size
- Type `.Machine` in the console and look for `double.eps`
