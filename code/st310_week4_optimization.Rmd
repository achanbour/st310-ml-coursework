---
title: "Optimization for machine learning"
author: "Anastasia Chanbour"
date : "22/10/2021"
output: html_document
---
# Load the required packages
```{r}
library(tidyverse) # required
library(broom)     # required
library(modelr)    # for data_grid
library(GGally)
theme_set(theme_minimal(base_size = 22))
```


# 1-d smooth regression example

## Example data

Generate a one-dimensional example with a non-linear relationship

```{r}
f <- function(x) sin(4*pi*x)
n <- 200
train1d <-
  data.frame(
    x = rbeta(n, 1, 3)
    ) %>%
  # change the noise level sd

  mutate(y = f(x) + rnorm(n, sd = .1))

# plot data and loess curve

ggplot(train1d, aes(x, y)) +
  geom_point() +
  geom_smooth()
```

Linear regression with a polynomial transform of x

```{r}
model_lm <- lm(y ~ poly(x, 5), data = train1d)

train1d_grid <-
  data_grid(train1d,
          x = seq_range(c(x, 1), 500, expand = .05))

augment(model_lm,
        newdata = train1d_grid) %>%
  ggplot(aes(x, y)) +
  geom_point(data = train1d) +
  geom_line(aes(y = .fitted))
```

## Gradient descent

Goal: implement gradient descent and use it to solve for the coefficients of the above linear model

Update is

$$
\beta_{k+1}=\beta_{k}-\gamma\nabla L(\beta_k)
$$

where $\gamma>0$ is the step size.

### Step 1: writing functions to output the least squares loss and its gradient

```{r}
# Loss function in linear regression RSS:
least_squares_loss <- function(x,y,beta) {
  sum((y-x%*%beta)^2)
}

# Loss function gradient:
least_squares_gradient <- function(x,y,beta) {
  -2*t(x)%*%(y-x%*%beta) #in matrix notation
}
```

### Step 2: writing a loop to take multiple steps in the direction of the negative gradient, keeping step size fixed

```{r}
# Model data and initialise coefficient vector
y <- train1d$y
x <- model.matrix(model_lm)
p <- ncol(x)

gamma <- 1 #step size within permitted values
beta0 <-rep(0,p) #initialise vector of parameters as 1xp vector of zeros

previous_loss <- least_squares_loss(x,y,beta0)
grad0 <- least_squares_gradient(x,y, beta0)
beta1 <- beta0 - gamma*grad0 #first update of beta
next_loss <- least_squares_loss(x,y,beta1)
previous_beta <- beta1

for(i in 1:5){
  gradn <- least_squares_gradient(x,y, previous_beta)
  next_beta <- previous_beta - gamma*gradn
  previous_beta <- next_beta
  print(previous_beta)
}

```

### Step 3: writing a function to step in the direction of the negative gradient until the loss function no longer decreases by a certain amount, keeping step size fixed

```{r}
gamma <- 0.001
beta0 <- beta0 <-rep(0,p)
previous_loss<- previous_loss <- least_squares_loss(x,y,beta0)
grad0 <- least_squares_gradient(x,y, beta0)
beta1 <- beta0 - gamma*grad0
next_loss <- least_squares_loss(x,y,beta1)
steps <- 1 #for printing updates


#we iterate until the change in values of the loss function is small enough
while ((next_loss - previous_loss > 0.01)) {
   gradn <- least_squares_gradient(x,y, previous_beta)
  next_beta <- previous_beta - gamma*gradn
  grad_current <- gradn
  if (steps %% 50 == 0) print(previous_loss)
  steps <- steps + 1
  previous_beta <- next_beta
  previous_loss <- next_loss
  next_loss <- least_squares_loss(x,y,next_beta)

}

```

### Step 4: experimenting with manually decreasing the stepsize and convergence threshold

```{r}
c(steps, previous_loss, next_loss)
next_beta
```

### Extra reference: use the Barzilai-Borwein method to choose step size

See https://en.wikipedia.org/wiki/Gradient_descent
