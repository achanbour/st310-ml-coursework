---
title: "Logistic regression"
author: "Anastasia Chanbour"
date:  "15/10/2021"
output: html_document
---
# Load the required packages
```{r}
library(tidyverse)
library(broom)
library(modeldata)
data(attrition)  #package with data
```

# Aim: Classification with logistic regression

## Part 1: Categorical outcome data

Comparing the distribution of a numeric predictor variable between the two outcome classes

```{r}
ggplot(attrition, aes(x=Attrition, y = TotalWorkingYears))+
  geom_boxplot()
```


Testing for difference in means

```{r}
t.test(TotalWorkingYears~Attrition, data = attrition)
```

Checking for class balance:

```{r}
attrition %>% count(Attrition)
table(attrition$Attrition)
```

Creating a balanced dataset with the same number of observations in both classes

Reason: Classifiers (such as Logistic Regression) tend to ignore small classes while concentrating on classifying the large ones accurately

```{r}
attr_No <- attrition %>%
  filter(Attrition == "No") %>%
  sample_n(size = 237)

attr_Yes <- attrition %>%
  filter(Attrition == "Yes")

attr <- rbind(attr_No, attr_Yes)

# or

attr <- attrition %>%
  group_by(Attrition) %>%
  slice_sample(n = 237)


# transform outcome to numeric 0-1
nattr <- attr %>%
  mutate(Y = as.numeric(Attrition) - 1) %>%
  select(Y, TotalWorkingYears)
```


## Classification: linear regression with the linear probability model (LPM)?

Plotting linear regression line, change the threshold

```{r}
ggplot(nattr, aes(x = TotalWorkingYears, y = Y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_hline(yintercept = .4) # or e.g. mean(nattr$Y)
```

Problems:

- Can predict outside 0-1 range
- Not directly interpretable as probabilities

### Thresholding ideas

Choose a threshold/cutoff value for predictor $X$, say $c$, and then classify

- $\hat Y = 1$ if $X \geq c$
- $\hat Y = 0$ otherwise

Or if the association is negative, change the sign

As we vary $c$, we trade-off between kinds of errors: false positives and false negatives

In the simple case with thresholding one predictor, the classification/decision rules are all equivalent whether we use linear regression or logistic regression (as long as the fitted relationship is monotone)

For **multiple** regression--when we have more predictors--we can then transform a numeric prediction from the model $\hat Y$ to a classification by using a threshold rule on the scale of the predictions (instead of on the scale of one predictor as before)

- $\hat Y = 1$ if $x^T \hat \beta \geq c$
- $\hat Y = 0$ otherwise

## Logistic regression

```{r}
model_glm <- glm(Y~TotalWorkingYears,data = nattr, family = binomial)
model_glm
```
# Compare the fit of the glm to LPM
```{r}
augment(model_glm, type.predict = "response") %>%
  ggplot(aes(TotalWorkingYears, Y)) +
  geom_point() +
  geom_line(aes(y = .fitted))
```

### Modeling assumption

$$
\text{logit}[P(Y = 1|X)] = \beta_0 + \beta_1 X
$$
some function (logit) of the mean of Y is equal to a linear function in X
$$
P(Y = 1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
$$
